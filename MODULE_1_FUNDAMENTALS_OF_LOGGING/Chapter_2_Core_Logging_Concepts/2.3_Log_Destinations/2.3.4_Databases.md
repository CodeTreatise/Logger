# 2.3.4 Databases

## Understanding Database-Based Logging

Database logging provides structured storage, advanced querying capabilities, and transactional integrity for log data. This section covers enterprise-grade database logging implementations, optimization strategies, and best practices for various database systems.

## Advanced Database Logging Implementation

### 1. **Universal Database Logger**
Enterprise-grade database logging system supporting multiple database backends with advanced features.

```javascript
// Advanced database logging implementation
const { EventEmitter } = require('events');

class AdvancedDatabaseLogger extends EventEmitter {
  constructor(options = {}) {
    super();
    
    this.options = {
      database: options.database || 'postgresql', // 'postgresql', 'mysql', 'mongodb', 'elasticsearch', 'cassandra'
      connection: options.connection || {},
      
      // Table/Collection settings
      tableName: options.tableName || 'application_logs',
      schema: options.schema || 'public',
      createTable: options.createTable !== false,
      
      // Performance settings
      batchSize: options.batchSize || 1000,
      batchTimeout: options.batchTimeout || 5000,
      maxRetries: options.maxRetries || 3,
      retryDelay: options.retryDelay || 1000,
      
      // Connection pool settings
      poolSize: options.poolSize || 10,
      poolTimeout: options.poolTimeout || 30000,
      idleTimeout: options.idleTimeout || 300000,
      
      // Data management
      partitioning: options.partitioning || null,
      indexing: options.indexing !== false,
      compression: options.compression || false,
      ttl: options.ttl || null, // Time to live in seconds
      
      // Query optimization
      queryTimeout: options.queryTimeout || 30000,
      maxQuerySize: options.maxQuerySize || 10000,
      
      // Security
      encryption: options.encryption || false,
      sanitization: options.sanitization !== false,
      
      // Monitoring
      enableMetrics: options.enableMetrics !== false,
      slowQueryThreshold: options.slowQueryThreshold || 1000,
      
      ...options
    };
    
    this.dbClient = null;
    this.connectionPool = null;
    this.batchQueue = [];
    this.batchTimer = null;
    this.metrics = this.initializeMetrics();
    this.schemaManager = this.setupSchemaManager();
    this.queryBuilder = this.setupQueryBuilder();
    this.optimizationManager = this.setupOptimizationManager();
    this.partitionManager = this.setupPartitionManager();
    
    this.initialize();
  }
  
  initializeMetrics() {
    return {
      totalLogs: 0,
      batchesProcessed: 0,
      queriesExecuted: 0,
      slowQueries: 0,
      connectionErrors: 0,
      queryErrors: 0,
      averageInsertTime: 0,
      averageQueryTime: 0,
      connectionsCreated: 0,
      connectionsDestroyed: 0,
      diskSpaceUsed: 0,
      indexHits: 0,
      indexMisses: 0,
      partitionsCreated: 0,
      startTime: Date.now()
    };
  }
  
  setupSchemaManager() {
    return {
      // PostgreSQL schema
      postgresql: {
        createTableSQL: () => `
          CREATE TABLE IF NOT EXISTS ${this.options.schema}.${this.options.tableName} (
            id BIGSERIAL PRIMARY KEY,
            timestamp TIMESTAMPTZ NOT NULL DEFAULT NOW(),
            level VARCHAR(10) NOT NULL,
            logger VARCHAR(255),
            message TEXT NOT NULL,
            metadata JSONB,
            hostname VARCHAR(255),
            pid INTEGER,
            thread_id VARCHAR(50),
            request_id VARCHAR(100),
            user_id VARCHAR(100),
            session_id VARCHAR(100),
            correlation_id VARCHAR(100),
            stack_trace TEXT,
            source_file VARCHAR(500),
            source_line INTEGER,
            created_at TIMESTAMPTZ NOT NULL DEFAULT NOW(),
            updated_at TIMESTAMPTZ NOT NULL DEFAULT NOW()
          );
        `,
        
        createIndexesSQL: () => [
          `CREATE INDEX IF NOT EXISTS idx_${this.options.tableName}_timestamp ON ${this.options.schema}.${this.options.tableName} (timestamp DESC);`,
          `CREATE INDEX IF NOT EXISTS idx_${this.options.tableName}_level ON ${this.options.schema}.${this.options.tableName} (level);`,
          `CREATE INDEX IF NOT EXISTS idx_${this.options.tableName}_logger ON ${this.options.schema}.${this.options.tableName} (logger);`,
          `CREATE INDEX IF NOT EXISTS idx_${this.options.tableName}_request_id ON ${this.options.schema}.${this.options.tableName} (request_id);`,
          `CREATE INDEX IF NOT EXISTS idx_${this.options.tableName}_user_id ON ${this.options.schema}.${this.options.tableName} (user_id);`,
          `CREATE INDEX IF NOT EXISTS idx_${this.options.tableName}_correlation_id ON ${this.options.schema}.${this.options.tableName} (correlation_id);`,
          `CREATE INDEX IF NOT EXISTS idx_${this.options.tableName}_metadata ON ${this.options.schema}.${this.options.tableName} USING gin (metadata);`
        ],
        
        insertSQL: () => `
          INSERT INTO ${this.options.schema}.${this.options.tableName} 
          (timestamp, level, logger, message, metadata, hostname, pid, thread_id, 
           request_id, user_id, session_id, correlation_id, stack_trace, source_file, source_line)
          VALUES ($1, $2, $3, $4, $5, $6, $7, $8, $9, $10, $11, $12, $13, $14, $15)
        `,
        
        batchInsertSQL: (count) => {
          const values = [];
          let paramIndex = 1;
          
          for (let i = 0; i < count; i++) {
            const params = [];
            for (let j = 0; j < 15; j++) {
              params.push(`$${paramIndex++}`);
            }
            values.push(`(${params.join(', ')})`);
          }
          
          return `
            INSERT INTO ${this.options.schema}.${this.options.tableName} 
            (timestamp, level, logger, message, metadata, hostname, pid, thread_id, 
             request_id, user_id, session_id, correlation_id, stack_trace, source_file, source_line)
            VALUES ${values.join(', ')}
          `;
        }
      },
      
      // MySQL schema
      mysql: {
        createTableSQL: () => `
          CREATE TABLE IF NOT EXISTS \`${this.options.tableName}\` (
            id BIGINT UNSIGNED AUTO_INCREMENT PRIMARY KEY,
            timestamp DATETIME(6) NOT NULL DEFAULT CURRENT_TIMESTAMP(6),
            level VARCHAR(10) NOT NULL,
            logger VARCHAR(255),
            message TEXT NOT NULL,
            metadata JSON,
            hostname VARCHAR(255),
            pid INT,
            thread_id VARCHAR(50),
            request_id VARCHAR(100),
            user_id VARCHAR(100),
            session_id VARCHAR(100),
            correlation_id VARCHAR(100),
            stack_trace TEXT,
            source_file VARCHAR(500),
            source_line INT,
            created_at DATETIME(6) NOT NULL DEFAULT CURRENT_TIMESTAMP(6),
            updated_at DATETIME(6) NOT NULL DEFAULT CURRENT_TIMESTAMP(6) ON UPDATE CURRENT_TIMESTAMP(6),
            INDEX idx_timestamp (timestamp DESC),
            INDEX idx_level (level),
            INDEX idx_logger (logger),
            INDEX idx_request_id (request_id),
            INDEX idx_user_id (user_id),
            INDEX idx_correlation_id (correlation_id)
          ) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_unicode_ci;
        `,
        
        createIndexesSQL: () => [],
        
        insertSQL: () => `
          INSERT INTO \`${this.options.tableName}\` 
          (timestamp, level, logger, message, metadata, hostname, pid, thread_id, 
           request_id, user_id, session_id, correlation_id, stack_trace, source_file, source_line)
          VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
        `
      },
      
      // MongoDB schema
      mongodb: {
        createCollection: async () => {
          if (!this.dbClient.db().collection(this.options.tableName)) {
            await this.dbClient.db().createCollection(this.options.tableName);
          }
        },
        
        createIndexes: async () => {
          const collection = this.dbClient.db().collection(this.options.tableName);
          
          await collection.createIndexes([
            { key: { timestamp: -1 } },
            { key: { level: 1 } },
            { key: { logger: 1 } },
            { key: { 'metadata.request_id': 1 } },
            { key: { 'metadata.user_id': 1 } },
            { key: { 'metadata.correlation_id': 1 } },
            { key: { 'metadata.hostname': 1 } },
            { key: { timestamp: 1, level: 1 } }, // Compound index
            { key: { 'metadata.$**': 1 } } // Wildcard index for metadata
          ]);
        }
      }
    };
  }
  
  setupQueryBuilder() {
    return {
      // Build query for log retrieval
      buildSelectQuery: (filters, options = {}) => {
        const database = this.options.database;
        
        switch (database) {
          case 'postgresql':
            return this.queryBuilder.buildPostgreSQLQuery(filters, options);
          case 'mysql':
            return this.queryBuilder.buildMySQLQuery(filters, options);
          case 'mongodb':
            return this.queryBuilder.buildMongoDBQuery(filters, options);
          default:
            throw new Error(`Unsupported database: ${database}`);
        }
      },
      
      buildPostgreSQLQuery: (filters, options) => {
        let query = `SELECT * FROM ${this.options.schema}.${this.options.tableName}`;
        const params = [];
        let paramIndex = 1;
        const whereConditions = [];
        
        // Build WHERE conditions
        if (filters.level) {
          whereConditions.push(`level = $${paramIndex++}`);
          params.push(filters.level);
        }
        
        if (filters.logger) {
          whereConditions.push(`logger = $${paramIndex++}`);
          params.push(filters.logger);
        }
        
        if (filters.startTime) {
          whereConditions.push(`timestamp >= $${paramIndex++}`);
          params.push(filters.startTime);
        }
        
        if (filters.endTime) {
          whereConditions.push(`timestamp <= $${paramIndex++}`);
          params.push(filters.endTime);
        }
        
        if (filters.message) {
          whereConditions.push(`message ILIKE $${paramIndex++}`);
          params.push(`%${filters.message}%`);
        }
        
        if (filters.metadata) {
          Object.entries(filters.metadata).forEach(([key, value]) => {
            whereConditions.push(`metadata->>'${key}' = $${paramIndex++}`);
            params.push(value);
          });
        }
        
        if (whereConditions.length > 0) {
          query += ` WHERE ${whereConditions.join(' AND ')}`;
        }
        
        // Add ORDER BY
        query += ` ORDER BY timestamp ${options.order || 'DESC'}`;
        
        // Add LIMIT
        if (options.limit) {
          query += ` LIMIT $${paramIndex++}`;
          params.push(options.limit);
        }
        
        // Add OFFSET
        if (options.offset) {
          query += ` OFFSET $${paramIndex++}`;
          params.push(options.offset);
        }
        
        return { query, params };
      },
      
      buildMySQLQuery: (filters, options) => {
        let query = `SELECT * FROM \`${this.options.tableName}\``;
        const params = [];
        const whereConditions = [];
        
        // Build WHERE conditions
        if (filters.level) {
          whereConditions.push('level = ?');
          params.push(filters.level);
        }
        
        if (filters.logger) {
          whereConditions.push('logger = ?');
          params.push(filters.logger);
        }
        
        if (filters.startTime) {
          whereConditions.push('timestamp >= ?');
          params.push(filters.startTime);
        }
        
        if (filters.endTime) {
          whereConditions.push('timestamp <= ?');
          params.push(filters.endTime);
        }
        
        if (filters.message) {
          whereConditions.push('message LIKE ?');
          params.push(`%${filters.message}%`);
        }
        
        if (filters.metadata) {
          Object.entries(filters.metadata).forEach(([key, value]) => {
            whereConditions.push('JSON_EXTRACT(metadata, ?) = ?');
            params.push(`$.${key}`, value);
          });
        }
        
        if (whereConditions.length > 0) {
          query += ` WHERE ${whereConditions.join(' AND ')}`;
        }
        
        // Add ORDER BY
        query += ` ORDER BY timestamp ${options.order || 'DESC'}`;
        
        // Add LIMIT
        if (options.limit) {
          query += ' LIMIT ?';
          params.push(options.limit);
          
          if (options.offset) {
            query += ' OFFSET ?';
            params.push(options.offset);
          }
        }
        
        return { query, params };
      },
      
      buildMongoDBQuery: (filters, options) => {
        const query = {};
        
        if (filters.level) {
          query.level = filters.level;
        }
        
        if (filters.logger) {
          query.logger = filters.logger;
        }
        
        if (filters.startTime || filters.endTime) {
          query.timestamp = {};
          if (filters.startTime) {
            query.timestamp.$gte = new Date(filters.startTime);
          }
          if (filters.endTime) {
            query.timestamp.$lte = new Date(filters.endTime);
          }
        }
        
        if (filters.message) {
          query.message = { $regex: filters.message, $options: 'i' };
        }
        
        if (filters.metadata) {
          Object.entries(filters.metadata).forEach(([key, value]) => {
            query[`metadata.${key}`] = value;
          });
        }
        
        return {
          query,
          options: {
            sort: { timestamp: options.order === 'ASC' ? 1 : -1 },
            limit: options.limit || 0,
            skip: options.offset || 0
          }
        };
      }
    };
  }
  
  setupOptimizationManager() {
    return {
      // Query performance analysis
      analyzeQuery: async (query, params, executionTime) => {
        if (executionTime > this.options.slowQueryThreshold) {
          this.metrics.slowQueries++;
          
          console.warn('Slow query detected:', {
            query: query.substring(0, 200),
            executionTime,
            params: params ? params.slice(0, 5) : []
          });
          
          this.emit('slowQuery', {
            query,
            params,
            executionTime,
            timestamp: new Date()
          });
        }
      },
      
      // Index usage analysis
      analyzeIndexUsage: async () => {
        const database = this.options.database;
        
        try {
          let indexStats;
          
          switch (database) {
            case 'postgresql':
              indexStats = await this.optimizationManager.getPostgreSQLIndexStats();
              break;
            case 'mysql':
              indexStats = await this.optimizationManager.getMySQLIndexStats();
              break;
            case 'mongodb':
              indexStats = await this.optimizationManager.getMongoDBIndexStats();
              break;
            default:
              return null;
          }
          
          this.emit('indexAnalysis', indexStats);
          return indexStats;
        } catch (error) {
          console.error('Index analysis failed:', error);
          return null;
        }
      },
      
      getPostgreSQLIndexStats: async () => {
        const query = `
          SELECT 
            schemaname,
            tablename,
            indexname,
            idx_tup_read,
            idx_tup_fetch,
            idx_scan
          FROM pg_stat_user_indexes 
          WHERE tablename = $1
        `;
        
        const result = await this.dbClient.query(query, [this.options.tableName]);
        return result.rows;
      },
      
      getMySQLIndexStats: async () => {
        const query = `
          SELECT 
            table_name,
            index_name,
            cardinality,
            sub_part,
            packed,
            nullable
          FROM information_schema.statistics 
          WHERE table_name = ? AND table_schema = DATABASE()
        `;
        
        const [rows] = await this.dbClient.execute(query, [this.options.tableName]);
        return rows;
      },
      
      getMongoDBIndexStats: async () => {
        const collection = this.dbClient.db().collection(this.options.tableName);
        return await collection.indexStats().toArray();
      },
      
      // Connection pool optimization
      optimizeConnectionPool: () => {
        const stats = this.connectionPool ? this.connectionPool.getStats() : null;
        
        if (stats) {
          const utilizationRate = stats.activeConnections / stats.totalConnections;
          
          if (utilizationRate > 0.8) {
            console.warn('High connection pool utilization:', stats);
            this.emit('highConnectionUtilization', stats);
          }
          
          if (stats.queuedRequests > 10) {
            console.warn('High connection queue length:', stats);
            this.emit('highConnectionQueue', stats);
          }
        }
      }
    };
  }
  
  setupPartitionManager() {
    return {
      // Create time-based partitions
      createTimePartitions: async (startDate, endDate, interval = 'month') => {
        const database = this.options.database;
        
        if (database !== 'postgresql' && database !== 'mysql') {
          console.warn(`Partitioning not supported for ${database}`);
          return;
        }
        
        const partitions = this.partitionManager.generatePartitionList(startDate, endDate, interval);
        
        for (const partition of partitions) {
          try {
            await this.partitionManager.createPartition(partition);
            this.metrics.partitionsCreated++;
          } catch (error) {
            console.error(`Failed to create partition ${partition.name}:`, error);
          }
        }
      },
      
      generatePartitionList: (startDate, endDate, interval) => {
        const partitions = [];
        const current = new Date(startDate);
        
        while (current <= endDate) {
          const next = new Date(current);
          
          switch (interval) {
            case 'day':
              next.setDate(next.getDate() + 1);
              break;
            case 'week':
              next.setDate(next.getDate() + 7);
              break;
            case 'month':
              next.setMonth(next.getMonth() + 1);
              break;
            case 'year':
              next.setFullYear(next.getFullYear() + 1);
              break;
          }
          
          partitions.push({
            name: `${this.options.tableName}_${this.formatPartitionDate(current, interval)}`,
            startDate: new Date(current),
            endDate: new Date(next)
          });
          
          current.setTime(next.getTime());
        }
        
        return partitions;
      },
      
      formatPartitionDate: (date, interval) => {
        const year = date.getFullYear();
        const month = String(date.getMonth() + 1).padStart(2, '0');
        const day = String(date.getDate()).padStart(2, '0');
        
        switch (interval) {
          case 'day':
            return `${year}${month}${day}`;
          case 'week':
            const week = Math.ceil(date.getDate() / 7);
            return `${year}${month}w${week}`;
          case 'month':
            return `${year}${month}`;
          case 'year':
            return `${year}`;
          default:
            return `${year}${month}`;
        }
      },
      
      createPartition: async (partition) => {
        const database = this.options.database;
        
        switch (database) {
          case 'postgresql':
            return this.partitionManager.createPostgreSQLPartition(partition);
          case 'mysql':
            return this.partitionManager.createMySQLPartition(partition);
          default:
            throw new Error(`Partitioning not supported for ${database}`);
        }
      },
      
      createPostgreSQLPartition: async (partition) => {
        const query = `
          CREATE TABLE IF NOT EXISTS ${this.options.schema}.${partition.name} 
          PARTITION OF ${this.options.schema}.${this.options.tableName}
          FOR VALUES FROM ('${partition.startDate.toISOString()}') 
          TO ('${partition.endDate.toISOString()}')
        `;
        
        await this.dbClient.query(query);
        console.log(`Created PostgreSQL partition: ${partition.name}`);
      },
      
      createMySQLPartition: async (partition) => {
        // MySQL partitioning requires different approach
        const query = `
          ALTER TABLE \`${this.options.tableName}\` 
          ADD PARTITION (
            PARTITION ${partition.name} VALUES LESS THAN (TO_DAYS('${partition.endDate.toISOString().split('T')[0]}'))
          )
        `;
        
        await this.dbClient.execute(query);
        console.log(`Created MySQL partition: ${partition.name}`);
      },
      
      // Automatic partition maintenance
      maintainPartitions: async () => {
        if (!this.options.partitioning) return;
        
        const now = new Date();
        const futureDate = new Date(now.getTime() + (30 * 24 * 60 * 60 * 1000)); // 30 days ahead
        
        await this.partitionManager.createTimePartitions(
          now,
          futureDate,
          this.options.partitioning.interval || 'month'
        );
        
        // Cleanup old partitions if TTL is set
        if (this.options.ttl) {
          await this.partitionManager.cleanupOldPartitions();
        }
      },
      
      cleanupOldPartitions: async () => {
        const cutoffDate = new Date(Date.now() - (this.options.ttl * 1000));
        
        // Implementation depends on database system
        console.log(`Cleaning up partitions older than ${cutoffDate.toISOString()}`);
      }
    };
  }
  
  async initialize() {
    try {
      // Initialize database connection
      await this.initializeConnection();
      
      // Setup schema
      if (this.options.createTable) {
        await this.setupSchema();
      }
      
      // Setup partitioning if enabled
      if (this.options.partitioning) {
        await this.partitionManager.maintainPartitions();
        
        // Schedule periodic partition maintenance
        setInterval(() => {
          this.partitionManager.maintainPartitions();
        }, 24 * 60 * 60 * 1000); // Daily
      }
      
      // Setup batch processing
      this.setupBatchProcessing();
      
      // Setup monitoring
      if (this.options.enableMetrics) {
        this.setupMonitoring();
      }
      
      console.log(`Database logger initialized: ${this.options.database}`);
      
    } catch (error) {
      console.error('Failed to initialize database logger:', error);
      throw error;
    }
  }
  
  async initializeConnection() {
    const database = this.options.database;
    
    switch (database) {
      case 'postgresql':
        await this.initializePostgreSQL();
        break;
      case 'mysql':
        await this.initializeMySQL();
        break;
      case 'mongodb':
        await this.initializeMongoDB();
        break;
      default:
        throw new Error(`Unsupported database: ${database}`);
    }
  }
  
  async initializePostgreSQL() {
    const { Pool } = require('pg');
    
    this.connectionPool = new Pool({
      ...this.options.connection,
      max: this.options.poolSize,
      idleTimeoutMillis: this.options.idleTimeout,
      connectionTimeoutMillis: this.options.poolTimeout
    });
    
    this.dbClient = this.connectionPool;
    
    // Test connection
    const client = await this.connectionPool.connect();
    await client.query('SELECT NOW()');
    client.release();
    
    this.metrics.connectionsCreated++;
  }
  
  async initializeMySQL() {
    const mysql = require('mysql2/promise');
    
    this.connectionPool = mysql.createPool({
      ...this.options.connection,
      connectionLimit: this.options.poolSize,
      acquireTimeout: this.options.poolTimeout,
      timeout: this.options.queryTimeout
    });
    
    this.dbClient = this.connectionPool;
    
    // Test connection
    const connection = await this.connectionPool.getConnection();
    await connection.execute('SELECT NOW()');
    connection.release();
    
    this.metrics.connectionsCreated++;
  }
  
  async initializeMongoDB() {
    const { MongoClient } = require('mongodb');
    
    this.dbClient = new MongoClient(this.options.connection.url, {
      ...this.options.connection,
      maxPoolSize: this.options.poolSize,
      serverSelectionTimeoutMS: this.options.poolTimeout,
      socketTimeoutMS: this.options.queryTimeout
    });
    
    await this.dbClient.connect();
    
    // Test connection
    await this.dbClient.db().admin().ping();
    
    this.metrics.connectionsCreated++;
  }
  
  async setupSchema() {
    const database = this.options.database;
    const schemaManager = this.schemaManager[database];
    
    if (!schemaManager) {
      throw new Error(`Schema management not supported for ${database}`);
    }
    
    try {
      if (database === 'mongodb') {
        await schemaManager.createCollection();
        
        if (this.options.indexing) {
          await schemaManager.createIndexes();
        }
      } else {
        // SQL databases
        await this.dbClient.query(schemaManager.createTableSQL());
        
        if (this.options.indexing) {
          const indexes = schemaManager.createIndexesSQL();
          for (const indexSQL of indexes) {
            await this.dbClient.query(indexSQL);
          }
        }
      }
      
      console.log(`Schema setup completed for ${database}`);
    } catch (error) {
      console.error('Schema setup failed:', error);
      throw error;
    }
  }
  
  setupBatchProcessing() {
    this.batchProcessor = {
      add: (logEntry) => {
        this.batchQueue.push(logEntry);
        
        if (this.batchQueue.length >= this.options.batchSize) {
          this.processBatch();
        } else if (!this.batchTimer) {
          this.batchTimer = setTimeout(() => {
            this.processBatch();
          }, this.options.batchTimeout);
        }
      },
      
      process: async () => {
        if (this.batchQueue.length === 0) return;
        
        const batch = [...this.batchQueue];
        this.batchQueue = [];
        
        if (this.batchTimer) {
          clearTimeout(this.batchTimer);
          this.batchTimer = null;
        }
        
        try {
          await this.insertBatch(batch);
          this.metrics.batchesProcessed++;
        } catch (error) {
          console.error('Batch processing failed:', error);
          this.metrics.queryErrors++;
          
          // Re-queue failed batch
          this.batchQueue.unshift(...batch);
        }
      }
    };
  }
  
  setupMonitoring() {
    // Emit metrics every minute
    setInterval(() => {
      this.emitMetrics();
    }, 60000);
    
    // Monitor connection pool every 30 seconds
    setInterval(() => {
      this.optimizationManager.optimizeConnectionPool();
    }, 30000);
    
    // Analyze index usage every hour
    setInterval(() => {
      this.optimizationManager.analyzeIndexUsage();
    }, 3600000);
  }
  
  async log(level, message, metadata = {}) {
    const logEntry = {
      timestamp: new Date(),
      level: level.toUpperCase(),
      logger: metadata.logger || 'default',
      message,
      metadata: this.sanitizeMetadata(metadata),
      hostname: require('os').hostname(),
      pid: process.pid,
      thread_id: metadata.thread_id,
      request_id: metadata.request_id,
      user_id: metadata.user_id,
      session_id: metadata.session_id,
      correlation_id: metadata.correlation_id,
      stack_trace: metadata.stack_trace,
      source_file: metadata.source_file,
      source_line: metadata.source_line
    };
    
    this.batchProcessor.add(logEntry);
    this.metrics.totalLogs++;
  }
  
  async insertBatch(batch) {
    const startTime = Date.now();
    const database = this.options.database;
    
    try {
      switch (database) {
        case 'postgresql':
          await this.insertPostgreSQLBatch(batch);
          break;
        case 'mysql':
          await this.insertMySQLBatch(batch);
          break;
        case 'mongodb':
          await this.insertMongoDBBatch(batch);
          break;
        default:
          throw new Error(`Unsupported database: ${database}`);
      }
      
      const executionTime = Date.now() - startTime;
      this.updateInsertMetrics(executionTime);
      
    } catch (error) {
      this.metrics.queryErrors++;
      throw error;
    }
  }
  
  async insertPostgreSQLBatch(batch) {
    const schemaManager = this.schemaManager.postgresql;
    const sql = schemaManager.batchInsertSQL(batch.length);
    const params = [];
    
    batch.forEach(entry => {
      params.push(
        entry.timestamp,
        entry.level,
        entry.logger,
        entry.message,
        JSON.stringify(entry.metadata),
        entry.hostname,
        entry.pid,
        entry.thread_id,
        entry.request_id,
        entry.user_id,
        entry.session_id,
        entry.correlation_id,
        entry.stack_trace,
        entry.source_file,
        entry.source_line
      );
    });
    
    await this.dbClient.query(sql, params);
  }
  
  async insertMySQLBatch(batch) {
    const schemaManager = this.schemaManager.mysql;
    const sql = schemaManager.insertSQL();
    
    const values = batch.map(entry => [
      entry.timestamp,
      entry.level,
      entry.logger,
      entry.message,
      JSON.stringify(entry.metadata),
      entry.hostname,
      entry.pid,
      entry.thread_id,
      entry.request_id,
      entry.user_id,
      entry.session_id,
      entry.correlation_id,
      entry.stack_trace,
      entry.source_file,
      entry.source_line
    ]);
    
    // Use batch insert for better performance
    const batchSQL = sql.replace('VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)', 
      `VALUES ${values.map(() => '(?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)').join(', ')}`);
    
    const flatParams = values.flat();
    await this.dbClient.execute(batchSQL, flatParams);
  }
  
  async insertMongoDBBatch(batch) {
    const collection = this.dbClient.db().collection(this.options.tableName);
    
    const documents = batch.map(entry => ({
      timestamp: entry.timestamp,
      level: entry.level,
      logger: entry.logger,
      message: entry.message,
      metadata: {
        ...entry.metadata,
        hostname: entry.hostname,
        pid: entry.pid,
        thread_id: entry.thread_id,
        request_id: entry.request_id,
        user_id: entry.user_id,
        session_id: entry.session_id,
        correlation_id: entry.correlation_id,
        stack_trace: entry.stack_trace,
        source_file: entry.source_file,
        source_line: entry.source_line
      }
    }));
    
    await collection.insertMany(documents, { ordered: false });
  }
  
  updateInsertMetrics(executionTime) {
    this.metrics.queriesExecuted++;
    
    const avgTime = this.metrics.averageInsertTime;
    const count = this.metrics.queriesExecuted;
    
    this.metrics.averageInsertTime = 
      (avgTime * (count - 1) + executionTime) / count;
    
    if (executionTime > this.options.slowQueryThreshold) {
      this.metrics.slowQueries++;
    }
  }
  
  sanitizeMetadata(metadata) {
    if (typeof metadata !== 'object' || metadata === null) {
      return {};
    }
    
    const sanitized = { ...metadata };
    const sensitiveKeys = ['password', 'token', 'apikey', 'secret', 'auth', 'authorization'];
    
    Object.keys(sanitized).forEach(key => {
      const lowerKey = key.toLowerCase();
      
      if (sensitiveKeys.some(sensitive => lowerKey.includes(sensitive))) {
        sanitized[key] = '[REDACTED]';
      }
    });
    
    return sanitized;
  }
  
  async processBatch() {
    return this.batchProcessor.process();
  }
  
  emitMetrics() {
    const uptime = Date.now() - this.metrics.startTime;
    const metricsSnapshot = {
      ...this.metrics,
      uptime,
      logsPerSecond: this.metrics.totalLogs / (uptime / 1000),
      batchesPerSecond: this.metrics.batchesProcessed / (uptime / 1000),
      averageBatchSize: this.metrics.totalLogs / Math.max(this.metrics.batchesProcessed, 1),
      errorRate: this.metrics.queryErrors / Math.max(this.metrics.queriesExecuted, 1) * 100,
      slowQueryRate: this.metrics.slowQueries / Math.max(this.metrics.queriesExecuted, 1) * 100
    };
    
    this.emit('metrics', metricsSnapshot);
  }
  
  // Query methods
  async query(filters, options = {}) {
    const startTime = Date.now();
    
    try {
      const database = this.options.database;
      let results;
      
      switch (database) {
        case 'postgresql':
        case 'mysql':
          const { query, params } = this.queryBuilder.buildSelectQuery(filters, options);
          const result = await this.dbClient.query(query, params);
          results = database === 'postgresql' ? result.rows : result[0];
          break;
          
        case 'mongodb':
          const { query, options: mongoOptions } = this.queryBuilder.buildSelectQuery(filters, options);
          const collection = this.dbClient.db().collection(this.options.tableName);
          results = await collection.find(query, mongoOptions).toArray();
          break;
          
        default:
          throw new Error(`Unsupported database: ${database}`);
      }
      
      const executionTime = Date.now() - startTime;
      this.updateQueryMetrics(executionTime);
      
      return results;
    } catch (error) {
      this.metrics.queryErrors++;
      throw error;
    }
  }
  
  updateQueryMetrics(executionTime) {
    this.metrics.queriesExecuted++;
    
    const avgTime = this.metrics.averageQueryTime;
    const count = this.metrics.queriesExecuted;
    
    this.metrics.averageQueryTime = 
      (avgTime * (count - 1) + executionTime) / count;
    
    this.optimizationManager.analyzeQuery('SELECT', [], executionTime);
  }
  
  // Convenience methods
  trace(message, metadata) { return this.log('trace', message, metadata); }
  debug(message, metadata) { return this.log('debug', message, metadata); }
  info(message, metadata) { return this.log('info', message, metadata); }
  warn(message, metadata) { return this.log('warn', message, metadata); }
  error(message, metadata) { return this.log('error', message, metadata); }
  fatal(message, metadata) { return this.log('fatal', message, metadata); }
  
  // Management methods
  async flush() {
    return this.processBatch();
  }
  
  getMetrics() {
    return { ...this.metrics };
  }
  
  async getHealth() {
    try {
      const database = this.options.database;
      let isHealthy = false;
      
      switch (database) {
        case 'postgresql':
          const pgResult = await this.dbClient.query('SELECT 1');
          isHealthy = pgResult.rows.length > 0;
          break;
          
        case 'mysql':
          const [mysqlResult] = await this.dbClient.execute('SELECT 1');
          isHealthy = mysqlResult.length > 0;
          break;
          
        case 'mongodb':
          await this.dbClient.db().admin().ping();
          isHealthy = true;
          break;
      }
      
      return {
        healthy: isHealthy,
        database: this.options.database,
        metrics: this.getMetrics(),
        queueSize: this.batchQueue.length
      };
    } catch (error) {
      return {
        healthy: false,
        error: error.message,
        database: this.options.database
      };
    }
  }
  
  // Cleanup
  async destroy() {
    try {
      // Flush remaining logs
      await this.flush();
      
      // Clear timers
      if (this.batchTimer) {
        clearTimeout(this.batchTimer);
      }
      
      // Close database connections
      if (this.dbClient) {
        if (this.options.database === 'mongodb') {
          await this.dbClient.close();
        } else if (this.connectionPool) {
          await this.connectionPool.end();
        }
      }
      
      console.log('Database logger destroyed');
    } catch (error) {
      console.error('Error during database logger destruction:', error);
    }
  }
}
```

### 2. **Database-Specific Optimizations**
Advanced optimization strategies for different database systems.

```javascript
// Database-specific optimization implementations
class DatabaseOptimizations {
  
  // PostgreSQL-specific optimizations
  static createPostgreSQLOptimizer(logger) {
    return {
      // Optimize for high-volume inserts
      setupOptimalConfiguration: async () => {
        const optimizationQueries = [
          // Increase checkpoint segments for better write performance
          "ALTER SYSTEM SET checkpoint_segments = 64",
          
          // Optimize shared buffers
          "ALTER SYSTEM SET shared_buffers = '256MB'",
          
          // Optimize WAL settings
          "ALTER SYSTEM SET wal_buffers = '16MB'",
          "ALTER SYSTEM SET wal_writer_delay = '200ms'",
          
          // Optimize background writer
          "ALTER SYSTEM SET bgwriter_delay = '200ms'",
          "ALTER SYSTEM SET bgwriter_lru_maxpages = 100",
          
          // Optimize autovacuum for log table
          `ALTER TABLE ${logger.options.schema}.${logger.options.tableName} 
           SET (autovacuum_vacuum_scale_factor = 0.05)`,
          
          `ALTER TABLE ${logger.options.schema}.${logger.options.tableName} 
           SET (autovacuum_analyze_scale_factor = 0.05)`
        ];
        
        for (const query of optimizationQueries) {
          try {
            await logger.dbClient.query(query);
          } catch (error) {
            console.warn('PostgreSQL optimization query failed:', error.message);
          }
        }
      },
      
      // Create specialized indexes for common query patterns
      createPerformanceIndexes: async () => {
        const indexes = [
          // Partial indexes for error levels
          `CREATE INDEX CONCURRENTLY IF NOT EXISTS idx_${logger.options.tableName}_errors 
           ON ${logger.options.schema}.${logger.options.tableName} (timestamp DESC, level) 
           WHERE level IN ('ERROR', 'FATAL')`,
          
          // Composite index for time-range queries
          `CREATE INDEX CONCURRENTLY IF NOT EXISTS idx_${logger.options.tableName}_time_level 
           ON ${logger.options.schema}.${logger.options.tableName} (timestamp DESC, level, logger)`,
          
          // Index for correlation queries
          `CREATE INDEX CONCURRENTLY IF NOT EXISTS idx_${logger.options.tableName}_correlation 
           ON ${logger.options.schema}.${logger.options.tableName} (correlation_id, timestamp) 
           WHERE correlation_id IS NOT NULL`,
          
          // Expression index for message search
          `CREATE INDEX CONCURRENTLY IF NOT EXISTS idx_${logger.options.tableName}_message_search 
           ON ${logger.options.schema}.${logger.options.tableName} 
           USING gin(to_tsvector('english', message))`
        ];
        
        for (const index of indexes) {
          try {
            await logger.dbClient.query(index);
          } catch (error) {
            console.warn('Index creation failed:', error.message);
          }
        }
      },
      
      // Implement table partitioning for better performance
      setupPartitioning: async () => {
        // Convert to partitioned table
        const partitionSQL = `
          -- Create new partitioned table
          CREATE TABLE ${logger.options.schema}.${logger.options.tableName}_partitioned (
            LIKE ${logger.options.schema}.${logger.options.tableName} INCLUDING ALL
          ) PARTITION BY RANGE (timestamp);
          
          -- Create monthly partitions for current and next year
          ${DatabaseOptimizations.generateMonthlyPartitions(logger.options.schema, logger.options.tableName + '_partitioned')}
          
          -- Migrate data (in production, use pg_dump/restore for large tables)
          INSERT INTO ${logger.options.schema}.${logger.options.tableName}_partitioned 
          SELECT * FROM ${logger.options.schema}.${logger.options.tableName};
          
          -- Rename tables
          ALTER TABLE ${logger.options.schema}.${logger.options.tableName} 
          RENAME TO ${logger.options.tableName}_backup;
          
          ALTER TABLE ${logger.options.schema}.${logger.options.tableName}_partitioned 
          RENAME TO ${logger.options.tableName};
        `;
        
        await logger.dbClient.query(partitionSQL);
      }
    };
  }
  
  // MongoDB-specific optimizations
  static createMongoDBOptimizer(logger) {
    return {
      setupOptimalConfiguration: async () => {
        const collection = logger.dbClient.db().collection(logger.options.tableName);
        
        // Create compound indexes for common query patterns
        await collection.createIndexes([
          // Time-based queries with level filtering
          { key: { timestamp: -1, level: 1 }, background: true },
          
          // Correlation tracking
          { key: { 'metadata.correlation_id': 1, timestamp: -1 }, 
            sparse: true, background: true },
          
          // User activity tracking
          { key: { 'metadata.user_id': 1, timestamp: -1 }, 
            sparse: true, background: true },
          
          // Text search on messages
          { key: { message: 'text' }, background: true },
          
          // TTL index for automatic cleanup
          { key: { timestamp: 1 }, 
            expireAfterSeconds: logger.options.ttl || 2592000, // 30 days
            background: true }
        ]);
      },
      
      // Implement sharding for horizontal scaling
      setupSharding: async (shardKey = { timestamp: 1, 'metadata.user_id': 1 }) => {
        const db = logger.dbClient.db();
        
        // Enable sharding on database
        await db.admin().command({ enableSharding: db.databaseName });
        
        // Shard the collection
        await db.admin().command({
          shardCollection: `${db.databaseName}.${logger.options.tableName}`,
          key: shardKey
        });
      },
      
      // Optimize for write-heavy workloads
      configureWriteConcern: () => {
        return {
          w: 1, // Acknowledge from primary only
          j: false, // Don't wait for journal
          wtimeout: 5000
        };
      }
    };
  }
  
  // MySQL-specific optimizations
  static createMySQLOptimizer(logger) {
    return {
      setupOptimalConfiguration: async () => {
        const optimizationQueries = [
          // Optimize for INSERT operations
          "SET GLOBAL innodb_buffer_pool_size = 268435456", // 256MB
          "SET GLOBAL innodb_log_file_size = 67108864", // 64MB
          "SET GLOBAL innodb_flush_log_at_trx_commit = 2",
          "SET GLOBAL sync_binlog = 0",
          
          // Optimize table for logging workload
          `ALTER TABLE \`${logger.options.tableName}\` 
           ENGINE=InnoDB 
           ROW_FORMAT=COMPRESSED 
           KEY_BLOCK_SIZE=8`
        ];
        
        for (const query of optimizationQueries) {
          try {
            await logger.dbClient.execute(query);
          } catch (error) {
            console.warn('MySQL optimization query failed:', error.message);
          }
        }
      },
      
      createPerformanceIndexes: async () => {
        const indexes = [
          // Covering index for common queries
          `CREATE INDEX idx_${logger.options.tableName}_covering 
           ON \`${logger.options.tableName}\` (timestamp, level, logger, message(100))`,
          
          // Functional index for JSON queries
          `CREATE INDEX idx_${logger.options.tableName}_request_id 
           ON \`${logger.options.tableName}\` ((JSON_EXTRACT(metadata, '$.request_id')))`,
          
          // Full-text index for message search
          `CREATE FULLTEXT INDEX idx_${logger.options.tableName}_message_fulltext 
           ON \`${logger.options.tableName}\` (message)`
        ];
        
        for (const index of indexes) {
          try {
            await logger.dbClient.execute(index);
          } catch (error) {
            console.warn('Index creation failed:', error.message);
          }
        }
      },
      
      setupPartitioning: async () => {
        // Implement range partitioning by month
        const partitionSQL = `
          ALTER TABLE \`${logger.options.tableName}\` 
          PARTITION BY RANGE (TO_DAYS(timestamp)) (
            ${DatabaseOptimizations.generateMySQLMonthlyPartitions()}
          )
        `;
        
        await logger.dbClient.execute(partitionSQL);
      }
    };
  }
  
  static generateMonthlyPartitions(schema, tableName) {
    const partitions = [];
    const currentDate = new Date();
    
    for (let i = -1; i <= 12; i++) {
      const partitionDate = new Date(currentDate.getFullYear(), currentDate.getMonth() + i, 1);
      const nextPartitionDate = new Date(currentDate.getFullYear(), currentDate.getMonth() + i + 1, 1);
      
      const partitionName = `${tableName}_${partitionDate.getFullYear()}_${String(partitionDate.getMonth() + 1).padStart(2, '0')}`;
      
      partitions.push(`
        CREATE TABLE ${schema}.${partitionName} 
        PARTITION OF ${schema}.${tableName}
        FOR VALUES FROM ('${partitionDate.toISOString()}') TO ('${nextPartitionDate.toISOString()}')
      `);
    }
    
    return partitions.join(';\n');
  }
  
  static generateMySQLMonthlyPartitions() {
    const partitions = [];
    const currentDate = new Date();
    
    for (let i = 1; i <= 12; i++) {
      const partitionDate = new Date(currentDate.getFullYear(), i, 1);
      const partitionName = `p${currentDate.getFullYear()}_${String(i).padStart(2, '0')}`;
      
      partitions.push(`
        PARTITION ${partitionName} VALUES LESS THAN (TO_DAYS('${partitionDate.toISOString().split('T')[0]}'))
      `);
    }
    
    return partitions.join(',\n');
  }
}
```

## Database Logging Best Practices

### **Production Database Logging Configuration**
```javascript
// Production-ready database logging setup
class ProductionDatabaseLogger {
  constructor(options = {}) {
    this.environment = process.env.NODE_ENV || 'development';
    this.databaseType = options.database || 'postgresql';
    
    // Configuration based on environment
    const config = this.getEnvironmentConfig();
    
    this.logger = new AdvancedDatabaseLogger({
      ...config,
      ...options,
      
      // Production optimizations
      batchSize: this.environment === 'production' ? 1000 : 100,
      batchTimeout: this.environment === 'production' ? 10000 : 5000,
      
      // Partitioning for production
      partitioning: this.environment === 'production' ? {
        enabled: true,
        interval: 'month',
        retention: 90 * 24 * 60 * 60 // 90 days
      } : null,
      
      // TTL for automatic cleanup
      ttl: this.environment === 'production' ? 
        90 * 24 * 60 * 60 : // 90 days in production
        7 * 24 * 60 * 60,   // 7 days in development
      
      // Security settings
      encryption: this.environment === 'production',
      sanitization: true
    });
    
    this.setupOptimizations();
    this.setupMonitoring();
    this.setupMaintenance();
  }
  
  getEnvironmentConfig() {
    const configs = {
      development: {
        connection: {
          host: 'localhost',
          port: 5432,
          database: 'app_logs_dev',
          user: 'logger_user',
          password: process.env.DB_PASSWORD
        },
        poolSize: 5,
        enableMetrics: true
      },
      
      testing: {
        connection: {
          host: 'localhost',
          port: 5432,
          database: 'app_logs_test',
          user: 'logger_user',
          password: process.env.DB_PASSWORD
        },
        poolSize: 3,
        enableMetrics: false,
        ttl: 24 * 60 * 60 // 1 day
      },
      
      production: {
        connection: {
          host: process.env.DB_HOST,
          port: parseInt(process.env.DB_PORT) || 5432,
          database: process.env.DB_NAME,
          user: process.env.DB_USER,
          password: process.env.DB_PASSWORD,
          ssl: {
            rejectUnauthorized: false
          }
        },
        poolSize: 20,
        enableMetrics: true,
        compression: true,
        indexing: true
      }
    };
    
    return configs[this.environment] || configs.development;
  }
  
  async setupOptimizations() {
    try {
      let optimizer;
      
      switch (this.databaseType) {
        case 'postgresql':
          optimizer = DatabaseOptimizations.createPostgreSQLOptimizer(this.logger);
          break;
        case 'mysql':
          optimizer = DatabaseOptimizations.createMySQLOptimizer(this.logger);
          break;
        case 'mongodb':
          optimizer = DatabaseOptimizations.createMongoDBOptimizer(this.logger);
          break;
        default:
          console.warn(`No optimizations available for ${this.databaseType}`);
          return;
      }
      
      if (this.environment === 'production') {
        await optimizer.setupOptimalConfiguration();
        await optimizer.createPerformanceIndexes();
        
        if (optimizer.setupPartitioning) {
          await optimizer.setupPartitioning();
        }
      }
      
      console.log(`Database optimizations applied for ${this.databaseType}`);
    } catch (error) {
      console.error('Failed to apply database optimizations:', error);
    }
  }
  
  setupMonitoring() {
    // Monitor database health
    setInterval(async () => {
      try {
        const health = await this.logger.getHealth();
        
        if (!health.healthy) {
          console.error('Database logger unhealthy:', health);
          this.emit('databaseUnhealthy', health);
        }
        
        // Check for performance issues
        const metrics = health.metrics;
        
        if (metrics.slowQueryRate > 10) {
          console.warn('High slow query rate detected:', metrics.slowQueryRate);
        }
        
        if (metrics.errorRate > 5) {
          console.warn('High error rate detected:', metrics.errorRate);
        }
        
        if (metrics.averageInsertTime > 1000) {
          console.warn('High insert latency detected:', metrics.averageInsertTime);
        }
        
      } catch (error) {
        console.error('Health check failed:', error);
      }
    }, 60000); // Every minute
    
    // Monitor query performance
    this.logger.on('slowQuery', (queryInfo) => {
      console.warn('Slow query detected:', {
        executionTime: queryInfo.executionTime,
        query: queryInfo.query.substring(0, 100) + '...'
      });
    });
    
    // Monitor metrics
    this.logger.on('metrics', (metrics) => {
      if (this.environment === 'production') {
        // Send metrics to monitoring system
        this.sendMetricsToMonitoring(metrics);
      }
    });
  }
  
  setupMaintenance() {
    // Daily maintenance tasks
    const maintenanceInterval = this.environment === 'production' ? 
      24 * 60 * 60 * 1000 : // Daily in production
      7 * 24 * 60 * 60 * 1000; // Weekly in development
    
    setInterval(async () => {
      try {
        await this.performMaintenance();
      } catch (error) {
        console.error('Maintenance failed:', error);
      }
    }, maintenanceInterval);
  }
  
  async performMaintenance() {
    console.log('Starting database maintenance...');
    
    try {
      // Analyze query performance
      await this.logger.optimizationManager.analyzeIndexUsage();
      
      // Update statistics (database-specific)
      if (this.databaseType === 'postgresql') {
        await this.logger.dbClient.query(`ANALYZE ${this.logger.options.schema}.${this.logger.options.tableName}`);
      } else if (this.databaseType === 'mysql') {
        await this.logger.dbClient.execute(`ANALYZE TABLE \`${this.logger.options.tableName}\``);
      }
      
      // Cleanup old partitions if TTL is set
      if (this.logger.options.ttl && this.logger.partitionManager) {
        await this.logger.partitionManager.cleanupOldPartitions();
      }
      
      console.log('Database maintenance completed successfully');
    } catch (error) {
      console.error('Database maintenance failed:', error);
    }
  }
  
  sendMetricsToMonitoring(metrics) {
    // Implementation would depend on monitoring system
    // Examples: DataDog, New Relic, Prometheus, etc.
    
    const monitoringData = {
      service: 'database-logger',
      environment: this.environment,
      database: this.databaseType,
      metrics: {
        'logs.total': metrics.totalLogs,
        'logs.per_second': metrics.logsPerSecond,
        'queries.total': metrics.queriesExecuted,
        'queries.slow_rate': metrics.slowQueryRate,
        'queries.error_rate': metrics.errorRate,
        'performance.insert_time_avg': metrics.averageInsertTime,
        'performance.query_time_avg': metrics.averageQueryTime,
        'batches.processed': metrics.batchesProcessed,
        'batches.size_avg': metrics.averageBatchSize
      },
      timestamp: Date.now()
    };
    
    // Send to monitoring service
    console.log('Metrics:', monitoringData);
  }
  
  // Logging interface
  async log(level, message, metadata = {}) {
    // Add production context
    const enrichedMetadata = {
      ...metadata,
      environment: this.environment,
      service: process.env.SERVICE_NAME || 'unknown',
      version: process.env.APP_VERSION || '1.0.0',
      instance: require('os').hostname()
    };
    
    return this.logger.log(level, message, enrichedMetadata);
  }
  
  // Convenience methods
  info(message, metadata) { return this.log('info', message, metadata); }
  warn(message, metadata) { return this.log('warn', message, metadata); }
  error(message, metadata) { return this.log('error', message, metadata); }
  debug(message, metadata) { return this.log('debug', message, metadata); }
  
  // Query interface
  async query(filters, options) {
    return this.logger.query(filters, options);
  }
  
  // Health check
  async getHealth() {
    return this.logger.getHealth();
  }
  
  // Cleanup
  async destroy() {
    await this.logger.destroy();
  }
}

// Usage example
const logger = new ProductionDatabaseLogger({
  database: 'postgresql',
  connection: {
    host: process.env.DB_HOST,
    port: 5432,
    database: 'app_logs',
    user: 'logger_user',
    password: process.env.DB_PASSWORD
  }
});

// Application logging
logger.info('Application started', { port: 3000 });
logger.error('Database connection failed', { 
  error: 'Connection timeout',
  retryAttempt: 3 
});

// Query logs
const recentErrors = await logger.query({
  level: 'ERROR',
  startTime: new Date(Date.now() - 24 * 60 * 60 * 1000) // Last 24 hours
}, {
  limit: 100,
  order: 'DESC'
});

// Health monitoring
const health = await logger.getHealth();
console.log('Database logger health:', health);
```

---

**Previous**: [2.3.3 Network Endpoints](./2.3.3_Network_Endpoints.md)  
**Next**: [2.3.5 Message Queues](./2.3.5_Message_Queues.md)
